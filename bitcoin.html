<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Predicting bitcoin opening price</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about_me.html">About Me</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Portfolio
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="wine-classification.html">Classifying wine quality</a>
    </li>
    <li>
      <a href="bitcoin.html">Predicting bitcoin value</a>
    </li>
    <li>
      <a href="imdb.html">Visualizing IMDB ratings</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Contact Me
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="mailto:jsteven.raquel8@gmail.com">jsteven.raquel8@gmail.com</a>
    </li>
    <li>
      <a href="https://github.com/jstevenr">GitHub</a>
    </li>
    <li>
      <a href="https://www.linkedin.com/in/jstevenr/">LinkedIn</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Predicting bitcoin opening price</h1>

</div>


<p>This was my final project for PSTAT 174, Time Series, Fall 2017.</p>
<div id="note-that-the-numbers-cited-in-the-documentation-might-be-slightly-off-due-to-changes-in-random-sampling-in-subsequent-runs-of-the-code." class="section level2">
<h2>Note that the numbers cited in the documentation might be slightly off due to changes in random sampling in subsequent runs of the code.</h2>
</div>
<div id="abstract" class="section level1">
<h1>Abstract</h1>
<p>This analysis attempts to build an accurate forecasting model for the opening price of the bitcoin cryptocurrency, using data from an entire year and the Box-Jenkins methodology. This process consisted of model identification (after Box-Cox transformation and differencing), model selection by comparing AICc, parameter estimation using maximum likelihood estimation, and finally model diagnostics from looking at the ACF and PACF of the residuals.</p>
<p>As it turns out, the two models with the lowest AICc, ARIMA(0,1,0) and ARIMA(2,1,0), were similarly flawed on account of the inherent heteroskedasticity in the original data, which was discovered when running model diagnostics. Ultimately it was decided that the ARIMA model was unfeasible for modeling this particular dataset and the GARCH model would be better utilized for this situation. Having said that, both models resulted in very similar results and confidence intervals that contain the true values, with the ARIMA(2,1,0) proving only slightly more effective when forecasting only 10 values into the future. However, given that the coefficients of this model were not significant and in keeping with the principle of parsimony, we would opt for the ARIMA(0,1,0) random model with drift, <span class="math inline">\(X_t = X_{t-1} + 0.0028 + Z_t\)</span>.</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This dataset is the opening price of the cryptocurrency bitcoin for 366 days, from Jun 26 2015 to Jun 26, 2016 (2016 being a leap year). Cryptocurrency is a digital currency, with bitcoin being the most well-known. It is untraceable and is known for its usage for purchasing illicit goods and services. With internet anonymity and privacy becoming more important in the modern day, the value of untraceable currency has skyrocketed in the past several years. I want to be able to forecast the value of bitcoin in the future, based off of its value in the past. The dataset was found on <a href="https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory/kernels">Kaggle.com</a>, which in turn got the data from <a href="https://coinmarketcap.com/">coinmarketcap.com</a>.</p>
</div>
<div id="body" class="section level1">
<h1>Body</h1>
<div id="preliminary-plots" class="section level2">
<h2>Preliminary Plots</h2>
<p>To first analyze the dataset, I want to read in the data and then plot its time series. The data is organized to where the values are the top are the most recent, and those at the bottom are the oldest, so I will have to reverse this. I’m going to subset the data to a certain range in the middle so that I can compare my forecasting with actual existing data. After doing this, I will plot the series along a trend line or ‘line of best fit’.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
bitcoin &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;bitcoin_price.csv&quot;</span>)
data &lt;-<span class="st"> </span><span class="kw">subset</span>(bitcoin, <span class="dt">select =</span> <span class="kw">c</span>(Date, Open))  <span class="co"># selecting only opening price column</span>
<span class="co"># flipping the data to go from oldest to newest</span>
data2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Date =</span> data<span class="op">$</span>Date[<span class="dv">1655</span><span class="op">:</span><span class="dv">1</span>], <span class="dt">Open =</span> data<span class="op">$</span>Open[<span class="dv">1655</span><span class="op">:</span><span class="dv">1</span>])
<span class="co"># taking a 356 observation subset of the data</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))
open &lt;-<span class="st"> </span>data2<span class="op">$</span>Open[<span class="dv">790</span><span class="op">:</span><span class="dv">1155</span>]
<span class="kw">plot.ts</span>(open, <span class="dt">main =</span> <span class="st">&quot;Opening Daily Price of Bitcoin&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;USD&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Time, in weeks&quot;</span>)
<span class="kw">abline</span>(<span class="kw">lm</span>(open <span class="op">~</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(open))), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend =</span> <span class="st">&quot;trend line&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/initial%20plot-1.png" width="672" /></p>
</div>
<div id="trend-seasonality-and-sharp-changes" class="section level2">
<h2>Trend, Seasonality and Sharp Changes</h2>
<p>There is a very clear upward trend in the data, several sharp changes, and the mean is in no way constant over time. All of these are red flags that the data is not stationary. I do not believe from looking at the data that seasonality is an issue since there is no consistent change that occurs periodically. The issue of the trend can and will be addressed via differencing, and similarly seasonality can be addressed in this way but we need not do that here. The sharp changes do make a difference in terms of the variance, which we will attempt to make constant via power transformations such as through the Box-Cox transformation.</p>
</div>
<div id="transforming" class="section level2">
<h2>Transforming</h2>
<p>With the end goal of forecasting, the first step after looking at the data is to transform it to make it stationary, i.e. make it so the future behaves as the past. In order to determine the best way to do that, we should look at the ACF (auto-covariance) and PACF (partial auto-covariance) plots, which depict how data are affected by the observations that precede them. This will also give us an idea of the model for the data.</p>
<p>The <em>autocorrelation</em> of data, represented by the function <span class="math inline">\(\gamma_X\)</span> for some data <span class="math inline">\(X\)</span>, is the correlation in the data between two different points in time. One of the conditions for stationarity of a dataset is for the autocorrelation between two points in time <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span> to be equal to the autocorrelation of the difference between <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The <em>partial autocorrelation</em> is the original ACF, controlling for the values of the time series at all shorter lags. Properties of these plots help us determine the order of the AR and MA components of our ARMA model.</p>
<p>AR (auto-regressive) models depend on their past values, an error term, and sometimes a constant. MA (moving average) models depend linearly on the current and past values of white noise error terms (which is random). The combination of them, together with differencing, consitute the ARIMA model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="co"># library(forecast) is loaded</span>
<span class="kw">Acf</span>(open, <span class="dt">main =</span> <span class="st">&quot;ACF of Original Data&quot;</span>, <span class="dt">lag.max =</span> <span class="dv">50</span>)
<span class="kw">Pacf</span>(open, <span class="dt">main =</span> <span class="st">&quot;PACF of Original Data&quot;</span>, <span class="dt">lag.max =</span> <span class="dv">50</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/ACF%20and%20PACF-1.png" width="672" /></p>
<p>The ACF of the data decays gradually, whereas the PACF cuts off abruptly. There are a few significant lags of the PACF at 4, 5, and 10, after which point the values can be taken as just zero. The gradual decay of the ACF tells us that we should difference the data, which would also address the issue of the upward trend.</p>
<div id="box-cox-transformation" class="section level3">
<h3>Box-Cox Transformation</h3>
<p>We would like to begin by using the Box-Cox power transformation, which is a method that helps to Normalize data. This method finds a value <span class="math inline">\(\lambda\)</span> which maximizes the log-likelihood and then the data is raised to the power of <span class="math inline">\(\lambda\)</span>. The higher the likelihood, the better (more likely) the model parameter. Power transformations such as the Box-Cox help to stabilize the variance which is imperative for Box-Jenkins modeling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))
<span class="co"># library(MASS) is loaded</span>
bcTransform &lt;-<span class="st"> </span><span class="kw">boxcox</span>(open <span class="op">~</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(open)))</code></pre></div>
<p><img src="bitcoin_files/figure-html/finding%20best%20lambda-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># value of lambda which maxes log-likelihood</span>
best.lambda &lt;-<span class="st"> </span>bcTransform<span class="op">$</span>x[<span class="kw">which</span>(bcTransform<span class="op">$</span>y <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(bcTransform<span class="op">$</span>y))]
best.lambda</code></pre></div>
<pre><code>## [1] 0.1818182</code></pre>
<p>We see that the ideal value for <span class="math inline">\(\lambda\)</span> that maximizes the likelihood is <span class="math inline">\(-0.020\)</span>, but the log function, <span class="math inline">\(\lambda = 0\)</span>, is within the confidence interval so we can use this instead. Here we will apply the transformation, then compare the variances of our original data with the transformed data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># transforming the data</span>
open.tr &lt;-<span class="st"> </span><span class="kw">log</span>(open)
<span class="co"># new time series plot</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))
<span class="kw">plot.ts</span>(open.tr, <span class="dt">main =</span> <span class="st">&quot;Series after Box-Cox&quot;</span>)
<span class="kw">abline</span>(<span class="kw">lm</span>(open.tr <span class="op">~</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(open.tr))), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/box-cox%20transformation-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># comparing the variances of original and transformed data</span>
<span class="kw">var</span>(open)</code></pre></div>
<pre><code>## [1] 14166.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(open.tr)</code></pre></div>
<pre><code>## [1] 0.06487496</code></pre>
<p>Obviously the data still has a linear trend, but the variance is much smaller. We haven’t differenced yet, so the upward trend is still apparent. The typical methodology is to difference at <span class="math inline">\(d = 1\)</span> when dealing with a linear upward trend such as this.</p>
</div>
<div id="differencing" class="section level3">
<h3>Differencing</h3>
<p>We only want to difference once and at lag 1 because we did not observe any seasonality when we first plotted the original time series. The differenced series will represent the <em>change</em> between observations that are <span class="math inline">\(d\)</span> units of time apart, in this case, <span class="math inline">\(d = 1\)</span>, which is to say one day apart. So our differenced (and transformed) data will represent the change from each value to the next, one day at a time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))
open.diff &lt;-<span class="st"> </span><span class="kw">diff</span>(open.tr, <span class="dt">lag =</span> <span class="dv">1</span>)
<span class="kw">plot.ts</span>(open.diff, <span class="dt">main =</span> <span class="st">&quot;Differencing at lag 1 after power&quot;</span>)
<span class="co"># plotting the trend line of the series</span>
<span class="kw">abline</span>(<span class="kw">lm</span>(open.diff <span class="op">~</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(open.diff))), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/differencing-1.png" width="672" /></p>
<p>Note that the y-intercept for the trend line is now approximately zero, so our upward trend has been nullified by the differencing. It does concern us that the variance does not appear to be constant throughout (heteroskedasticity) in spite of the Box-Cox transformation being applied, which is theoretically the best power transformation available. In the graph it is noted how the variance is extremely small at certain points, and at other times very large. We can however proceed with an augmented Dickey-Fuller test which will determine whether the data is stationary or not by checking against null hypothesis that the sample has a unit root. If it doesn’t, then it is stationary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(tseries) is loaded</span>
<span class="kw">adf.test</span>(open.diff, <span class="dt">alternative =</span> <span class="st">&quot;stationary&quot;</span>)</code></pre></div>
<pre><code>## Warning in adf.test(open.diff, alternative = &quot;stationary&quot;): p-value smaller than
## printed p-value</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  open.diff
## Dickey-Fuller = -7.0202, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<p>Our p-value being lower than confidence level <span class="math inline">\(\alpha = 0.05\)</span> gives us significant evidence to reject the null hypothesis that the data is not stationary, thereby concluding that it is. Satisfied by this, we can now move on with estimating the parameters of the model.</p>
</div>
</div>
<div id="parameter-estimation" class="section level2">
<h2>Parameter Estimation</h2>
<p>Now we want to try and estimate the order of the model i.e. values of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. We can start speculating on these values by plotting the ACF and PACF of the newly differenced series. We can get ideas on what the orders are by using the <code>auto.arima()</code> function on the original time series, as well as the ACF and PACF of our transformed series. We’ll include <span class="math inline">\(\lambda = 0\)</span> as a part of our function so that it estimates a model of our Box-Cox transformed data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(forecast) is loaded stepwise and approximation = F is slower, but more</span>
<span class="co"># accurate</span>
auto.fit &lt;-<span class="st"> </span><span class="kw">auto.arima</span>(open, <span class="dt">stepwise =</span> F, <span class="dt">approximation =</span> F, <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">ic =</span> <span class="st">&quot;aicc&quot;</span>)
auto.fit</code></pre></div>
<pre><code>## Series: open 
## ARIMA(0,1,4) with drift 
## Box Cox transformation: lambda= 0 
## 
## Coefficients:
##          ma1      ma2      ma3     ma4   drift
##       0.0462  -0.0766  -0.0212  0.1660  0.0026
## s.e.  0.0519   0.0513   0.0503  0.0556  0.0017
## 
## sigma^2 estimated as 0.000833:  log likelihood=778.57
## AIC=-1545.14   AICc=-1544.91   BIC=-1521.74</code></pre>
<p>Note the inclusion of a ‘drift’ constant in the model. The <code>auto.arima()</code> function only includes the drift if it improves the model, based on AICc, which is the information criterion we specified to judge the model on, something that will be explained in more detail later on. We note that the drift <span class="math inline">\(\mu = 0.0028\)</span> with standard error <span class="math inline">\(0.0016\)</span>, so the confidence interval for it actually includes zero hinting at this being a rather insignificant drift.</p>
<p>We are given an ARIMA(0,1,0) model which is essentially a random walk. We can take a look at the ACF and PACF curves as well, and if there are virtually no lags at which either coefficient is significantly far from zero, then it would corroborate this conclusion since it would indicate there is no AR or MA order in the model of best fit. The inclusion of <span class="math inline">\(d = 1\)</span> in the auto-fitted model also tells us that differencing at lag 1 was the right decision.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">Acf</span>(open.diff, <span class="dt">main =</span> <span class="st">&quot;ACF of transformed data&quot;</span>, <span class="dt">lag.max =</span> <span class="dv">50</span>)
<span class="kw">Pacf</span>(open.diff, <span class="dt">main =</span> <span class="st">&quot;PACF of transformed data&quot;</span>, <span class="dt">lag.max =</span> <span class="dv">50</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/ACF-PACF%20of%20differenced%20data-1.png" width="672" /></p>
<p>The ACF at lag 1 is not significantly far form zero, so we don’t difference further lest we overdifference the series.</p>
<p>Looking at 50 lags of this series, we see that there are no lags at which the ACF or PACF are significant. This is consistent with our automatically fitted model using <span class="math inline">\(p = 0\)</span> and <span class="math inline">\(q = 0\)</span>. This is a confidence interval however, and we do see that ACF and PACF are <em>nearly</em> significant at lag 2. A significant ACF at lag 2 would imply an MA component of order 2 (<span class="math inline">\(q = 2\)</span>), while a significant PACF at lag 2 would imply an AR component of order 2 (<span class="math inline">\(p = 2\)</span>) so it’s important we don’t rule them out completely.</p>
<div id="akaike-information-criterion" class="section level3">
<h3>Akaike Information Criterion</h3>
<p>AICc is a refinement of the AIC (Akaike Information Criterion), which is a formula based on the number of estimated parameters as well as the maximum likelihood estimator. The smaller the AIC, the stronger the model. The AICc corrects for finite sample sizes, addressing an issue with the AIC which can result in overfitting models, so we prefer it when fitting for an ARIMA model.</p>
<p>It is however important to note that the AIC (and AICc) depend on a model being univariate, linear, and with normally-distributed residuals. We know the first two are true from what we have done so far, but are unsure about the latter condition. It is something that we will diagnose in the model diagnostics.</p>
</div>
</div>
<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>For now we will just use the AICc to select a couple of models and determine later on if the residuals will be an issue.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># matrix of AIC values for p and q library(qpcR) and library(forecast) are loaded</span>
AICc.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="dv">3</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;p =&quot;</span>, <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>), 
    <span class="kw">paste</span>(<span class="st">&quot;q =&quot;</span>, <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>)))

<span class="co"># simulating arima models for p,q = 0,1,2,3,4 and d = 1</span>
<span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>) {
    <span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>) {
        AICc.mat[p <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, q <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">AICc</span>(<span class="kw">Arima</span>(open, <span class="dt">order =</span> <span class="kw">c</span>(p, <span class="dv">1</span>, q), <span class="dt">method =</span> <span class="st">&quot;ML&quot;</span>, 
            <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">include.constant =</span> T))
    }
}
<span class="co"># printing matrix</span>
AICc.mat</code></pre></div>
<pre><code>##           q = 0     q = 1     q = 2
## p = 0 -1542.541 -1540.874 -1540.398
## p = 1 -1540.821 -1539.224 -1538.392
## p = 2 -1540.862 -1538.822 -1541.783</code></pre>
<p>We see that the model with the <em>lowest</em> AICC is the ARIMA(0,1,0) model with an AICc of <span class="math inline">\(-1520.377\)</span>, but all of them are very close to one another. As expected, the next strongest model was the ARIMA(2,1,0) model, with an AICc statistic of <span class="math inline">\(-1519.999\)</span>. These being the strongest models are consistent with the fact that the ACF and PACF values were not significantly far from zero.</p>
<p>We want to estimate as few parameters as possible due to the principle of parsimony (to select as few parameters as possible). In our case, the more parameters there are, the weaker the model. All signs point to a random walk being the best fit for this model thus far, but we can fit all both, run diagnostics and compare. We’re also going to make sure we include the drift constant because it was determined that including it does lower the AICc even though it is not significantly different from zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ARIMA(0,1,0)</span>
fit010 &lt;-<span class="st"> </span><span class="kw">Arima</span>(open, <span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">method =</span> <span class="st">&quot;ML&quot;</span>, <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">include.constant =</span> T)
fit010</code></pre></div>
<pre><code>## Series: open 
## ARIMA(0,1,0) with drift 
## Box Cox transformation: lambda= 0 
## 
## Coefficients:
##        drift
##       0.0026
## s.e.  0.0015
## 
## sigma^2 estimated as 0.0008484:  log likelihood=773.28
## AIC=-1542.55   AICc=-1542.52   BIC=-1534.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ARIMA(2,1,0)</span>
fit210 &lt;-<span class="st"> </span><span class="kw">Arima</span>(open, <span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">method =</span> <span class="st">&quot;ML&quot;</span>, <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">include.constant =</span> T)
fit210</code></pre></div>
<pre><code>## Series: open 
## ARIMA(2,1,0) with drift 
## Box Cox transformation: lambda= 0 
## 
## Coefficients:
##          ar1      ar2   drift
##       0.0309  -0.0751  0.0026
## s.e.  0.0521   0.0521  0.0015
## 
## sigma^2 estimated as 0.0008475:  log likelihood=774.46
## AIC=-1540.93   AICc=-1540.82   BIC=-1525.33</code></pre>
<p>So, we have the ARIMA(0,1,0) with drift model <span class="math display">\[X_t = X_{t-1} + 0.0028 + Z_t\]</span> and the ARIMA(2,1,0) with drift model <span class="math display">\[X_t - 0.0471X_{t-1} + 0.0907X_{t-2} = Z_t + 0.0027\]</span></p>
<p>where <span class="math inline">\(Z_t\)</span> should be approximately white noise with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2_Z\)</span>. It’s important to note that in the case of the ARIMA(2,1,0) model the confidence interval of the MA and AR coefficients contain zero so the model is not significantly different from a random walk. As we noted before in our automatically fit model that resulted from <code>auto.arima()</code>, the drift is not significant, but as we noted earlier it does lower the AICc to some extent so we prefer to retain it. The drift <span class="math inline">\(\mu\)</span> represents the average day-to-day change (after the power transformation) which is approximately 0.0028 in both cases.</p>
</div>
<div id="model-diagnostics" class="section level2">
<h2>Model Diagnostics</h2>
<p>For us to proceed with forecasting with these ARIMA models, the residuals must be IID (independent, identically distributed) Gaussian white noise–that is, there is no trend, no seasonality and no change in variance, as well as a sample mean of approximately zero, and a variance of approximately 1.</p>
<p>We can conduct the Ljung-Box test, the McLeod-Li test, and the Shapiro-Wilk test for normality on our model’s residuals for model diagnostics.</p>
<p>The Ljung-Box test tests some data against the null hypothesis that the data <em>are</em> independently distributed. The McLeod-Li test is the same as conducting the Ljung-Box test, but on the <em>squared</em> residuals instead. The implication of rejecting the null in the McLeod-Li test but passing the Ljung-Box test is an inherent problem of non-constant variance (heteroskedasticity), which is something we noted before in preliminary plots before and after transforming and differencing. The Shapiro-Wilk tests against the null hypothesis that the data are Normal, the other important condition of an ARIMA model’s validity. Here we create a matrix comparing the p-values of each of the hypothesis tests on each model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resids010 &lt;-<span class="st"> </span>fit010<span class="op">$</span>residuals
resids210 &lt;-<span class="st"> </span>fit210<span class="op">$</span>residuals
k &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="kw">length</span>(open)))
<span class="co"># a matrix comparing p-values for the three different tests</span>
diag.mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&quot;ARIMA(0,1,0)&quot;</span>, 
    <span class="st">&quot;ARIMA(2,1,0)&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;Ljung-Box&quot;</span>, <span class="st">&quot;McLeod-Li&quot;</span>, <span class="st">&quot;Shapiro-Wilk&quot;</span>)))

<span class="co"># testing ARIMA(0,1,0) residuals</span>
diag.mat[<span class="dv">1</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">Box.test</span>(resids010, <span class="dt">type =</span> <span class="st">&quot;Ljung-Box&quot;</span>, <span class="dt">lag =</span> k, <span class="dt">fitdf =</span> <span class="dv">0</span>)<span class="op">$</span>p.value
diag.mat[<span class="dv">1</span>, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">Box.test</span>(resids010<span class="op">^</span><span class="dv">2</span>, <span class="dt">type =</span> <span class="st">&quot;Ljung-Box&quot;</span>, <span class="dt">lag =</span> k, <span class="dt">fitdf =</span> <span class="dv">0</span>)<span class="op">$</span>p.value
diag.mat[<span class="dv">1</span>, <span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">shapiro.test</span>(resids010)<span class="op">$</span>p.value
<span class="co"># testing ARIMA(2,1,0) residuals</span>
diag.mat[<span class="dv">2</span>, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">Box.test</span>(resids210, <span class="dt">type =</span> <span class="st">&quot;Ljung-Box&quot;</span>, <span class="dt">lag =</span> k, <span class="dt">fitdf =</span> <span class="dv">2</span>)<span class="op">$</span>p.value
diag.mat[<span class="dv">2</span>, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">Box.test</span>(resids210<span class="op">^</span><span class="dv">2</span>, <span class="dt">type =</span> <span class="st">&quot;Ljung-Box&quot;</span>, <span class="dt">lag =</span> k, <span class="dt">fitdf =</span> <span class="dv">2</span>)<span class="op">$</span>p.value
diag.mat[<span class="dv">2</span>, <span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">shapiro.test</span>(resids210)<span class="op">$</span>p.value
<span class="co"># matrix of p-values</span>
<span class="kw">round</span>(diag.mat, <span class="dv">3</span>)</code></pre></div>
<pre><code>##              Ljung-Box McLeod-Li Shapiro-Wilk
## ARIMA(0,1,0)     0.560         0            0
## ARIMA(2,1,0)     0.567         0            0</code></pre>
<p>In the case of both models, the residuals <em>fail to reject</em> the null hypothesis in the Ljung-Box test, but have overwhelming evidence to reject it in the case of the McLeod-Li and the Shapiro-Wilk test. All tests use an <span class="math inline">\(\alpha = 0.05\)</span> confidence level. These conclusions imply that while the residuals are all independently distributed, the squared residuals are not. They also tell us that the residuals are not Normally distributed. The fact that the squared residuals are correlated and the lack of normality are both major pitfalls in our ARIMA models, since our model selection and our forecasting are contingent on these assumptions.</p>
<p>Failing to reject the null hypothesis of the Ljung-Box test but rejecting the null hypothesis of the McLeod-Li test tells us that our data might be more typical of a GARCH (generalized auto-regressive conditional heteroskedasticity) model rather than ARIMA as we have fitted here. With the addendum that our model is not necessarily the best one available, we will run diagnostic checks on the rest of the model and proceed with forecasting anyway, just to see what happens.</p>
</div>
<div id="forecasting" class="section level2">
<h2>Forecasting</h2>
<p>Now we will proceed with forecasting the next 10 values in the time series using both the ARIMA(0,1,0) and the ARIMA(2,1,0) models, and compare the results with each other and with the true values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(forecast) is loaded</span>
forecast010 &lt;-<span class="st"> </span><span class="kw">forecast</span>(open, <span class="dt">model =</span> fit010, <span class="dt">lambda =</span> <span class="dv">0</span>)
forecast210 &lt;-<span class="st"> </span><span class="kw">forecast</span>(open, <span class="dt">model =</span> fit210, <span class="dt">lambda =</span> <span class="dv">0</span>)
true &lt;-<span class="st"> </span>data2<span class="op">$</span>Open[<span class="dv">1156</span><span class="op">:</span><span class="dv">1165</span>]  <span class="co"># the next ten true values</span>
open.add &lt;-<span class="st"> </span><span class="kw">c</span>(open, true)
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))
<span class="kw">plot.ts</span>(open.add[<span class="dv">326</span><span class="op">:</span><span class="dv">376</span>], <span class="dt">main =</span> <span class="st">&quot;True Values&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">800</span>), <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>, 
    <span class="dt">ylab =</span> <span class="st">&quot;USD&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Days from 6/26/15&quot;</span>)
<span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">labels =</span> <span class="kw">seq</span>(<span class="dv">330</span>, <span class="dv">370</span>, <span class="dv">10</span>), <span class="dt">at =</span> <span class="kw">seq</span>(<span class="dv">4</span>, <span class="dv">44</span>, <span class="dv">10</span>))
<span class="kw">plot</span>(forecast010, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">800</span>), <span class="dt">include =</span> <span class="dv">40</span>, <span class="dt">ylab =</span> <span class="st">&quot;USD&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Days from 6/26/15&quot;</span>, 
    <span class="dt">main =</span> <span class="st">&quot;ARIMA(0,1,0)&quot;</span>)
<span class="kw">plot</span>(forecast210, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">200</span>, <span class="dv">800</span>), <span class="dt">include =</span> <span class="dv">40</span>, <span class="dt">ylab =</span> <span class="st">&quot;USD&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Days from 6/26/15&quot;</span>, 
    <span class="dt">main =</span> <span class="st">&quot;ARIMA(2,1,0)&quot;</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/forecasting-1.png" width="672" /></p>
<p>As we can see, the ARIMA(2,1,0) is very similar to the ARIMA(0,1,0) due to the fact that the estimated coefficients of the ARIMA(2,1,0) model were not significantly different from zero. We do observe that while the forecasts predict a straight line, which the true values obviously did not follow, at the very least the confidence intervals of the predictions capture the true values. The further one goes into the future, the larger the confidence interval i.e. the more uncertain predictions become.</p>
<p>Here we print out two data frames, one for each model, comparing the true values, the predictions, the 95% confidence intervals, and the percent difference between the prediction and the true value (the column <code>pct.diff</code>). A value of <span class="math inline">\(4.43\)</span> means that the predicted value is 4.43% <em>bigger</em> than the true value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data.frame of ARIMA(0,1,0) forecasts</span>
df010 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">true =</span> true, <span class="dt">pred =</span> forecast010<span class="op">$</span>mean, <span class="dt">lower =</span> forecast010<span class="op">$</span>lower[, 
    <span class="dv">2</span>], <span class="dt">upper =</span> forecast010<span class="op">$</span>upper[, <span class="dv">2</span>], <span class="dt">pct.diff =</span> <span class="kw">round</span>((forecast010<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>true)<span class="op">/</span>true, 
    <span class="dv">4</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>)
df010</code></pre></div>
<pre><code>##      true     pred    lower    upper pct.diff
## 1  619.17 618.9224 584.5789 655.2837    -0.04
## 2  616.82 620.5089 572.3811 672.6836     0.60
## 3  619.24 622.0995 563.5298 686.7565     0.46
## 4  640.87 623.6942 556.3979 699.1300    -2.68
## 5  636.03 625.2929 550.3569 710.4321    -1.69
## 6  637.01 626.8957 545.0857 720.9843    -1.59
## 7  640.31 628.5027 540.3942 730.9767    -1.84
## 8  639.08 630.1137 536.1588 740.5331    -1.40
## 9  641.82 631.7289 532.2938 749.7390    -1.57
## 10 639.41 633.3482 528.7372 758.6567    -0.95</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data.frame of ARIMA(2,1,0) forecasts</span>
df210 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">true =</span> true, <span class="dt">pred =</span> forecast210<span class="op">$</span>mean, <span class="dt">lower =</span> forecast210<span class="op">$</span>lower[, 
    <span class="dv">2</span>], <span class="dt">upper =</span> forecast210<span class="op">$</span>upper[, <span class="dv">2</span>], <span class="dt">pct.diff =</span> <span class="kw">round</span>((forecast210<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>true)<span class="op">/</span>true, 
    <span class="dv">4</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>)
df210</code></pre></div>
<pre><code>##      true     pred    lower    upper pct.diff
## 1  619.17 619.1287 584.7909 655.4827    -0.01
## 2  616.82 620.4828 571.6623 673.4725     0.59
## 3  619.24 622.0502 563.7170 686.4198     0.45
## 4  640.87 623.6613 557.3232 697.8957    -2.69
## 5  636.03 625.2620 551.7738 708.5377    -1.69
## 6  637.01 626.8633 546.9005 718.5175    -1.59
## 7  640.31 628.4697 542.5561 727.9877    -1.85
## 8  639.08 630.0805 538.6271 737.0617    -1.41
## 9  641.82 631.6954 535.0368 745.8160    -1.58
## 10 639.41 633.3143 531.7300 754.3058    -0.95</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the average error</span>
<span class="kw">mean</span>(df010<span class="op">$</span>pct.diff)</code></pre></div>
<pre><code>## [1] -1.07</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(df210<span class="op">$</span>pct.diff)</code></pre></div>
<pre><code>## [1] -1.073</code></pre>
<p>On average, the predictions of the ARIMA(2,1,0) model were closer to the true values of the data, although it was very close. Having said that, we do know that the squared residuals of both of these models are not independent from each other (they have an auto-correlation) so we can’t technically declare either of these models truly valid. Also, we have only predicted 10 values into the future here so a larger sample size of predictions is necessary to determine which in the long-run is stronger. One would have to fit a GARCH model to account for the heteroskedasticity in the data, would probably result in a better fit or at least a model whose assumptions conform with the properties of the data.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Ultimately, our two lowest AICc models produced predictions that were relatively close to the true data, with the ARIMA(2,1,0) model <span class="math inline">\(X_t - 0.0471X_{t-1} + 0.0907X_{t-2} = Z_t + 0.0027\)</span> proving only slightly closer than the ARIMA(0,1,0) <span class="math inline">\(X_t = X_{t-1} + 0.0028 + Z_t\)</span>, but this was only for forecasting 10 values. Again it is important to note that both models did <em>not</em> pass model diagnostics and as such do not carry a lot of validity. If we had to choose between the models, we would opt for the ARIMA(0,1,0) because of the principle of parsimony which states we should estimate fewer values if we can.</p>
<p>This dataset was difficult to fit with an ARIMA model because of the fact that the variance was non-constant, which results in squared residuals that were dependent on one another. The residuals were also not Normal, even after transforming with the Box-Cox transformation in the beginning. Both issues easily apparent in our time series plot of the original data which has many sporadic bursts up and down. While the residuals themselves were not autocorrelated, this still does not make for a forecast-ready model, at least not with ARIMA.</p>
<p>If someone were to accurately forecast on this dataset, it would be prudent to utilize a GARCH model instead, which better takes into account the non-constant variance. GARCH is a typical model for financial data. Considering our dataset in question is concerning the value of cryptocurrency (which has been extremely variable in the past several years), prospective analysts would be best advised to utilize this model in the future.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>R Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
<p>Adrian Trapletti and Kurt Hornik (2017). tseries: Time Series Analysis and Computational Finance. R package version 0.10-42.</p>
<p>Hyndman RJ (2017). <em>forecast: Forecasting functions for time series and linear models</em>. R package version 8.2, &lt;URL: <a href="http://pkg.robjhyndman.com/forecast" class="uri">http://pkg.robjhyndman.com/forecast</a>&gt;.</p>
<p>Hyndman RJ and Khandakar Y (2008). “Automatic time series forecasting: the forecast package for R.” <em>Journal of Statistical Software</em>, <em>26</em>(3), pp. 1-22. &lt;URL: <a href="http://www.jstatsoft.org/article/view/v027i03" class="uri">http://www.jstatsoft.org/article/view/v027i03</a>&gt;.</p>
<p>Venables, W. N. &amp; Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0</p>
<p>David Stoffer (2016). astsa: Applied Statistical Time Series Analysis. R package version 1.7. <a href="https://CRAN.R-project.org/package=astsa" class="uri">https://CRAN.R-project.org/package=astsa</a></p>
<p>Andrej-Nikolai Spiess (2014). qpcR: Modelling and analysis of real-time PCR data. R package version 1.4-0. <a href="https://CRAN.R-project.org/package=qpcR" class="uri">https://CRAN.R-project.org/package=qpcR</a></p>
<p>Nau, Robert. “Statistical Forecasting: Notes on Regression and Time Series Analysis.”<br />
Statistical Forecasting: Notes on Regression and Time Series Analysis, Duke<br />
University, Sept. 2017, people.duke.edu/~rnau/411home.htm.</p>
<p>Shumway, Robert H., and David S. Stoffer. Time Series Analysis and Its Applications: with R Examples. 2nd ed., Springer, 2006.</p>
<p>Metcalfe, Andrew V., and Paul S.P. Cowpertwait. Introductory Time Series with R. Springer New York, 2009.</p>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pretty much all code is included in the body of the report</span>
fit210<span class="op">$</span>coef[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]  <span class="co"># the 2 AR coefficients</span></code></pre></div>
<pre><code>##         ar1         ar2 
##  0.03089711 -0.07507494</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))
<span class="co"># RESIDUALS residual plots have clear heteroskedasticity</span>
<span class="kw">plot.ts</span>(resids010, <span class="dt">main =</span> <span class="st">&quot;ARIMA(0,1,0) Residuals&quot;</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/spec.arma%20and%20plot.roots-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot.ts</span>(resids210, <span class="dt">main =</span> <span class="st">&quot;ARIMA(2,1,0) Residuals&quot;</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/spec.arma%20and%20plot.roots-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># QQ-plots of residuals</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))
<span class="kw">qqnorm</span>(resids010, <span class="dt">main =</span> <span class="st">&quot;Standardized Residuals of ARIMA(0,1,0)&quot;</span>)
<span class="kw">qqline</span>(resids010)
<span class="kw">qqnorm</span>(resids210, <span class="dt">main =</span> <span class="st">&quot;Standardized Residuals of ARIMA(2,1,0)&quot;</span>)
<span class="kw">qqline</span>(resids210)</code></pre></div>
<p><img src="bitcoin_files/figure-html/spec.arma%20and%20plot.roots-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ACF of residuals shows they are uncorrelated at every lag i.e. white noise</span>
<span class="kw">Acf</span>(resids010, <span class="dt">main =</span> <span class="st">&quot;ARIMA(0,1,0) Residuals&#39; ACF&quot;</span>)
<span class="kw">Acf</span>(resids210, <span class="dt">main =</span> <span class="st">&quot;ARIMA(2,1,0) Residuals&#39; ACF&quot;</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/spec.arma%20and%20plot.roots-4.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ACF of squared residuals</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))
<span class="co"># not uncorrelated, i.e. not white noise</span>
<span class="kw">Acf</span>(resids010<span class="op">^</span><span class="dv">2</span>, <span class="dt">main =</span> <span class="st">&quot;ARIMA(0,1,0) Squared Residuals&#39; ACF&quot;</span>)
<span class="kw">Acf</span>(resids210<span class="op">^</span><span class="dv">2</span>, <span class="dt">main =</span> <span class="st">&quot;ARIMA(2,1,0) Squared Residuals&#39; ACF&quot;</span>)</code></pre></div>
<p><img src="bitcoin_files/figure-html/spec.arma%20and%20plot.roots-5.png" width="672" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
